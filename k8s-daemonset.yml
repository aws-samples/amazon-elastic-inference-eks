# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# A copy of the License is located at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# or in the "license" file accompanying this file. This file is distributed
# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
# express or implied. See the License for the specific language governing
# permissions and limitations under the License.

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: inference-daemon
  name: inference-daemon
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: inference-daemon
  labels:
    app: inference-daemon
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: inference-daemon
  namespace: default
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: inference-daemon
spec:
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: inference-daemon
    spec:
      volumes:
      - name: config-volume
        configMap:
          name: inference-config
      hostNetwork: true
      nodeSelector:
        nodegroup: elastic-inference
      containers:
      - name: inference-daemon
        image: rnzdocker1/eks-elastic-inference-app:ea770a6a256bade77730501c9a71d5670f55887d
        imagePullPolicy: Always
        env:
        - name: SQS_TASK_QUEUE
          value: task-queue-eks-a
        - name: SQS_TASK_COMPLETED_QUEUE
          value: task-completed-queue-eks-a
        resources:
          requests:
            memory: 1024Mi
      - name: tensorflow-serving
        image: rnzdocker1/eks-elastic-inference-serving:8e6ac6052e2d45da060aa0d22e1945b3631c4c1b
        imagePullPolicy: Always
        resources:
          requests:
            memory: 1024Mi
        ports:
          - name: tf-serving-api
            containerPort: 8501
            hostPort: 8501
            protocol: TCP
        volumeMounts:
        - name: config-volume
          mountPath: /app/ei
          readOnly: true
---
# Configuration for the application
apiVersion: v1
kind: ConfigMap
metadata:
  name: inference-config
data:
  config.yaml: |-
    Version: 1
---
# k8s service definition
apiVersion: v1
kind: Service
metadata:
  name: inference-service
spec:
  selector:
    app: inference-daemon
  clusterIP: None
